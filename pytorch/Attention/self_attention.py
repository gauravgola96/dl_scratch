import torch.nn as nn

class AttentionHead(nn.Module):
    
    def __init__(self, d_model, d_feature, dropout=0.1):
        super().__init__()
        






